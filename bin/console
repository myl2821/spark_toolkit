#!/usr/bin/env ruby

require "bundler/setup"
require "spark_toolkit"

# You can add fixtures and/or initialization code here to make experimenting
# with your gem easier. You can also use a different console, if you like.

# (If you use this, don't forget to add pry to your Gemfile!)
# require "pry"
# Pry.start

require "pry"

conf = SparkToolkit::Conf::Configuration.new
conf.add_config_dir("config")
#conf["hadoop.security.authentication"] = "kerberos"

hdfs = SparkToolkit::HDFS::FileSystem.new("", conf)
#is = hdfs.open("/user/annoback/yyy.c")

yarn = SparkToolkit::YARN::Client.new conf
yarn.start

app = yarn.get_applications[-1]

id = app.get_application_id

args = ["--class", "org.apache.spark.deploy.PythonRunner",
        "--primary-py-file", "pi.py"
]

=begin
java.lang.System.setProperty("SPARK_YARN_MODE", "true")
sconf = org.apache.spark.SparkConf.new
sconf.set_spark_home(ENV['SPARK_HOME'])
sconf.set_master('yarn')
sconf.set_app_name('test')
sconf.set('spark.submit.deployMode', 'cluster')
sconf.set('spark.yarn.isPython', 'true')

#sconf.set_property('master', 'yarn')
cli_args=org.apache.spark.deploy.yarn.ClientArguments.new(args)
#client=org.apache.spark.deploy.yarn.Client.new(cli_args,conf,sconf)
#client.run
=end

sc = SparkToolkit::Spark::Client.new(conf)
sc.set_app_name 'fff'
sc.is_python_job(true)

binding.pry
